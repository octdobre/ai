# How LLMs predict the next word

Large language models (LLMs) predict the next word by estimating a probability distribution over the vocabulary conditioned on the prompt ðŸ§ ðŸ”¤. The input is tokenized, embedded, and processed by transformer layers with selfâ€‘attention, which lets the model focus on the most relevant context.

The network outputs logits for each token; softmax turns these into probabilities ðŸŽ¯. Decoding then selects the next token using greedy selection or sampling methods like temperature, topâ€‘k, or nucleus sampling ðŸŽ². The chosen token is appended and fed back, repeating step by step. Trained on vast corpora, LLMs internalize linguistic patterns, enabling coherent, contextually appropriate continuations ðŸ“š.

## Examples

- Greedy decoding:
```text
Prompt: The capital of France is
â†’ Paris
```

- Temperature = 1.0 (more diverse):
```text
Prompt: The story begins at midnight and
â†’ the city exhales; lanterns flicker as footsteps scatter into alleys.
```

- Topâ€‘k / nucleus sampling:
```text
Prompt: A recipe for pancakes includes
â†’ flour, eggs, milk, a pinch of salt, and butter.
```

## Further Reading ðŸ”—
- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)
- [The Curious Case of Neural Text Degeneration (Holtzman et al., 2020)](https://arxiv.org/abs/1904.09751)
